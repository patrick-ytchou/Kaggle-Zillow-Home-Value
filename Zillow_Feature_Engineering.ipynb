{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Feature Engineering\n",
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import warnings\n",
    "import time\n",
    "import numba as nb\n",
    "\n",
    "# display set up\n",
    "%precision 4\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer decorator\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('----------------------')\n",
    "            print('Function %r takes %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for memory reduction\n",
    "@nb.jit()\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m             )\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         ]\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mform_blocks\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FloatBlock\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         \u001b[0mfloat_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FloatBlock\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_multi_blockify\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   1818\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1820\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_stack_arrays\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mstacked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load in target dataa\n",
    "df_2016 = pd.read_csv('train_2016_v2.csv')\n",
    "df_2017 = pd.read_csv('train_2017.csv')\n",
    "\n",
    "# Load in properties data\n",
    "prop_2016 = pd.read_csv('properties_2016.csv')\n",
    "prop_2017 = pd.read_csv('properties_2017.csv')\n",
    "assert len(prop_2016) == len(prop_2017)\n",
    "print(f\"Number of properties: {len(prop_2016)}\".format(len(prop_2016)))\n",
    "print(f\"Number of property features: {len(prop_2016.columns)-1}\")\n",
    "\n",
    "# Reduce Size\n",
    "prop_2016 = reduce_mem_usage(prop_2016)\n",
    "prop_2017 = reduce_mem_usage(prop_2017)\n",
    "print(f\"Rows for prop 2016: {len(prop_2016)}\")\n",
    "print(f\"Rows for prop 2017: {len(prop_2017)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Engineering\n",
    "From what we have learned in the Exploratory Data Analysis part, we can do the following feature engineering process. Some Engineering process is from my personal exploratory analysis, and some others are from some fantastic and insightful kaggle notebooks learned from other machine learning experts.\n",
    "\n",
    "### Define useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns will be drop later.\n",
    "drop_list = ['parcelid','architecturalstyletypeid','buildingclasstypeid','calculatedbathnbr','decktypeid',\n",
    " 'finishedfloor1squarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13','finishedsquarefeet15',\n",
    " 'finishedsquarefeet6', 'fullbathcnt', 'regionidcounty', \n",
    " 'regionidneighborhood', 'typeconstructiontypeid', 'taxvaluedollarcnt', 'assessmentyear',\n",
    " 'censustractandblock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime(df):\n",
    "    dt = pd.to_datetime(df.transactiondate).dt\n",
    "    df['year'] = (dt.year - 2016).astype(int)\n",
    "    df['month'] = (dt.month).astype(int)\n",
    "    df['quarter'] = (dt.quarter).astype(int)\n",
    "    df.drop(['transactiondate'], axis=1, inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = extract_datetime(df_2016)\n",
    "df_2017 = extract_datetime(df_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Pattern\n",
    "\n",
    "Inspired by the post below that talks about the null pattern. There are indeed lots of null values in this dataset. We can simply create some vectors to recognize those null patterns, which may be helpful to know the overall structure of the dataset.\n",
    "https://www.kaggle.com/c/ieee-fraud-detection/discussion/108727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_null_pattern(df):\n",
    "    null_pattern = df.isnull().astype(int)\n",
    "    pca = PCA(n_components=10)\n",
    "    columns = ['NAPattern1','NAPattern2','NAPattern3','NAPattern4','NAPattern5','NAPattern6','NAPattern7',\n",
    "              'NAPattern8','NAPattern9','NAPattern10']\n",
    "    null_embedding = pd.DataFrame(pca.fit_transform(null_pattern), columns=columns)\n",
    "    df = pd.concat([df,null_embedding],axis=1)\n",
    "    return(df)\n",
    "\n",
    "prop_2016 = get_null_pattern(prop_2016)\n",
    "prop_2017 = get_null_pattern(prop_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "There are lots of missing values in the categorical columns. Let's encode them into one of the unseen value so that we can afterwards groupby those categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical(df, columns):\n",
    "    for col in columns:\n",
    "        df.loc[df[col].isnull(),col] = -1\n",
    "        df[col] = df[col].astype('category')\n",
    "    return(df)\n",
    "\n",
    "impute_columns = ['regionidzip','propertyzoningdesc','propertycountylandusecode','regionidcounty','regionidcity',\n",
    "                 'airconditioningtypeid','buildingqualitytypeid','fips','heatingorsystemtypeid']\n",
    "prop_2016 = impute_categorical(prop_2016, impute_columns)\n",
    "prop_2017 = impute_categorical(prop_2017, impute_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use latitude and longitude data to get some insights from the geo-location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_list = ['latitude', 'longitude']\n",
    "\n",
    "for col in geo_list:\n",
    "    prop_2016[col] = prop_2016[col].apply(lambda x: float(x))/10e6\n",
    "    prop_2016[col] = prop_2016[col].fillna(stats.mode(prop_2016[col])[0][0])\n",
    "    \n",
    "for col in geo_list:\n",
    "    prop_2017[col] = prop_2017[col].apply(lambda x: float(x))/10e6\n",
    "    prop_2017[col] = prop_2017[col].fillna(stats.mode(prop_2017[col])[0][0])\n",
    "    \n",
    "@timeit\n",
    "def get_optimal_geocluster(prop_2016, prop_2017):\n",
    "    K = range(2,16)\n",
    "    for df in [prop_2016, prop_2017]:\n",
    "        mapping_dav = {} \n",
    "\n",
    "        print(f'Start Fitting.')\n",
    "        for k in K: \n",
    "            #Building and fitting the model \n",
    "            kmeanModel = KMeans(n_clusters=k)\n",
    "            pred = kmeanModel.fit_predict(prop_2016[geo_list])     \n",
    "\n",
    "            dav = davies_bouldin_score(prop_2016[geo_list], pred)\n",
    "            print(dav)\n",
    "            mapping_dav[k] = dav\n",
    "\n",
    "            print(f\"Iteration {k} finished. The davies bouldin score for {k} cluster is: {np.round(dav,4)}\")\n",
    "        \n",
    "        print(f'The best 5 cluster number is '+ str(sorted(mapping_dav, key=mapping_dav.get, reverse=True)[:5]))\n",
    "        print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the lower the davie-bouldin index the better, we know that **k = 10** is the best number of clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_2016['geo_cluster'] = KMeans(n_clusters=10).fit_predict(prop_2016[geo_list])\n",
    "prop_2017['geo_cluster'] = KMeans(n_clusters=10).fit_predict(prop_2017[geo_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Insightful Transformation for geographical features \n",
    "\n",
    "## The location might be one important issue. \n",
    "## However, we don't know how the latitutde and longitude value is transformed here.\n",
    "## Let's do some basic transformation\n",
    "def get_rotated_coord(df):\n",
    "    ## Rotated Coordinates\n",
    "    df['coord_1'] = df['latitude'] + df['longitude']\n",
    "    df['coord_2'] = df['latitude'] - df['longitude']\n",
    "    df['coord_3'] = df['latitude'] + 2 * df['longitude']\n",
    "    df['coord_4'] = df['latitude'] - 2 * df['longitude']\n",
    "    return(df)\n",
    "\n",
    "prop_2016 = get_rotated_coord(prop_2016)    \n",
    "prop_2017 = get_rotated_coord(prop_2017) \n",
    "drop_list.extend(geo_list) # Drop those latitude and longitude data out.\n",
    "\n",
    "\n",
    "## Why NYC and Silicon Valley is very expensive? Because the demand is high!\n",
    "## Demand and supply is one important issue for house pricing.\n",
    "## Since the regioneighborhood has lots of missing value, here we use regionzip to define neighborhood instead \n",
    "def get_neighborhood(df):\n",
    "    df['regionidcity_groupcnt'] = df['regionidcity'].map(df['regionidcity'].value_counts())\n",
    "    return(df)\n",
    "\n",
    "prop_2016 = get_neighborhood(prop_2016)    \n",
    "prop_2017 = get_neighborhood(prop_2017) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned that there are features for bathroom and bedroom. We also have features for the total area for the entire house -- from there we can calculate the **average size of room**. \n",
    "\n",
    "There is a bathroom and bedroom feature. Why not construct a roomcount feature from that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_2016[['roomcnt','bedroomcnt','bathroomcnt']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_room(df):\n",
    "    ## Roomcount for each house\n",
    "    df['derived_room_cnt'] = df['bedroomcnt'] + df['bathroomcnt']\n",
    "\n",
    "    ## Why there's a difference? Maybe here's how the error comes from\n",
    "    df['diff_derived_roomcnt'] = df['derived_room_cnt'] - df['roomcnt']\n",
    "\n",
    "    ## Average Size for each room for house\n",
    "    mask = (df.derived_room_cnt >= 1)\n",
    "    df.loc[mask, 'avg_room_size'] = df.loc[mask, 'calculatedfinishedsquarefeet'] / df.loc[mask, 'derived_room_cnt']\n",
    "\n",
    "    ## Average Size for each garage\n",
    "    mask = (df.garagecarcnt >= 1)\n",
    "    df.loc[mask,'avg_garage_size'] = df.loc[mask, 'garagetotalsqft'] / df.loc[mask,'garagecarcnt']\n",
    "\n",
    "    ## Average Tax Rate\n",
    "    df['property_tax_per_sqft'] = df['taxamount'] / df['calculatedfinishedsquarefeet']\n",
    "    return(df)\n",
    "\n",
    "prop_2016 = get_aggregate_room(prop_2016)\n",
    "prop_2017 = get_aggregate_room(prop_2017)\n",
    "\n",
    "    \n",
    "# Average Size per room across each category\n",
    "def add_aggregate_across_categories(df, groupby_cols, aggregate_col):\n",
    "    dataframe = df.copy(deep=True)   \n",
    "    df = df[df[aggregate_col].notna()]\n",
    "    for col in groupby_cols:\n",
    "        temp = df.groupby(col)[aggregate_col].agg([np.mean])\n",
    "        temp.columns = [f'{aggregate_col}_across_{col}']\n",
    "        try:\n",
    "            dataframe = dataframe.merge(how='left', right=temp, on=f'{col}')\n",
    "        except:\n",
    "            temp[f'{col}'] = temp[f'{col}'].astype(np.str)\n",
    "            dataframe = dataframe.merge(how='left', right=temp, on=f'{col}')\n",
    "    \n",
    "    for col in groupby_cols:\n",
    "        mean = dataframe[f'{aggregate_col}_across_{col}']\n",
    "        diff = dataframe[col] - mean\n",
    "        \n",
    "        dataframe[f'{aggregate_col}_across_{col}_diff'] = diff\n",
    "        dataframe[f'{aggregate_col}_across_{col}_percent'] = diff / mean\n",
    "    \n",
    "    return(dataframe)\n",
    "\n",
    "groupby_cols = ['airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid',\n",
    "                'propertycountylandusecode','propertylandusetypeid','propertyzoningdesc',\n",
    "                'rawcensustractandblock', 'regionidcity','regionidneighborhood','regionidzip',\n",
    "                'yearbuilt','assessmentyear','taxdelinquencyyear','censustractandblock','geo_cluster']\n",
    "\n",
    "aggregate_col = 'avg_room_size'\n",
    "\n",
    "prop_2016 = add_aggregate_across_categories(prop_2016, groupby_cols, aggregate_col)\n",
    "prop_2017 = add_aggregate_across_categories(prop_2017, groupby_cols, aggregate_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_aggregate_across_geo(df, groupby_col, aggregate_cols):\n",
    "    dataframe = df.copy(deep=True)\n",
    "    new_columns = []  # New feature columns added to the DataFrame\n",
    "\n",
    "    for col in aggregate_cols:\n",
    "        temp = df.groupby(groupby_col, as_index=False)[col].agg([np.mean])\n",
    "        temp.columns = [f'{col}_across_{groupby_col}']\n",
    "        new_columns += list(temp.columns)\n",
    "        try:\n",
    "            dataframe = dataframe.merge(how='left', right=temp, on=f'{groupby_col}')\n",
    "        except:\n",
    "            temp[f'{col}'] = temp[f'{col}'].astype(np.str)\n",
    "            dataframe = dataframe.merge(how='left', right=temp, on=f'{groupby_col}')\n",
    "    \n",
    "     for col in groupby_cols:\n",
    "        mean = dataframe[f'{col}_across_{groupby_col}']\n",
    "        diff = dataframe[col] - mean\n",
    "        \n",
    "        dataframe[f'{col}_across_{groupby_col}_diff'] = diff\n",
    "        if col != 'year_built':\n",
    "            dataframe[f'{col}_across_{groupby_col}_percent'] = diff / mean\n",
    "    \n",
    "    return(dataframe)\n",
    "\n",
    "groupby_col = 'regionidzip'\n",
    "aggregate_cols = ['lotsizesquarefeet', 'calculatedfinishedsquarefeet', 'yearbuilt'\n",
    "            'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount', 'property_tax_per_sqft']\n",
    "\n",
    "prop_2016 = add_aggregate_across_geo(prop_2016, groupby_col, aggregate_cols)\n",
    "prop_2017 = add_aggregate_across_geo(prop_2017, groupby_col, aggregate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_2016[['censustractandblock','rawcensustractandblock']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_alignment(df):\n",
    "    return(((df['censustractandblock'] / 10e5) > df['rawcensustractandblock']).apply(int))\n",
    "\n",
    "prop_2016 = get_census_alignment(prop_2016)\n",
    "prop_2017 = get_census_alignment(prop_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 689.20 Mb (39.9% reduction)\n",
      "Mem. usage decreased to 683.50 Mb (40.4% reduction)\n",
      "Number of properties: 2985217\n",
      "Number of property features: 91\n",
      "CPU times: user 5min 33s, sys: 4.49 s, total: 5min 37s\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Drop unneeded columns\n",
    "prop_2016 = prop_2016.drop(columns=drop_list)\n",
    "prop_2017 = prop_2017.drop(columns=drop_list)\n",
    "\n",
    "# Reduce Size\n",
    "prop_2016 = reduce_mem_usage(prop_2016)\n",
    "prop_2017 = reduce_mem_usage(prop_2017)\n",
    "\n",
    "print(f\"Number of properties: {len(prop_2016)}\".format(len(prop_2016)))\n",
    "print(f\"Number of property features: {len(prop_2016.columns)-1}\")\n",
    "\n",
    "# Write feature DataFrames to csv\n",
    "prop_2016.to_csv('data/prop_2016.csv', index=False)\n",
    "prop_2017.to_csv('data/prop_2017.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training set size: 167888\n"
     ]
    }
   ],
   "source": [
    "# Merge Dataset\n",
    "df_prop_2016 = pd.merge(df_2016,prop_2016,how='left',on='parcelid')\n",
    "df_prop_2017 = pd.merge(df_2017,prop_2017,how='left',on='parcelid')\n",
    "\n",
    "# Combine the 2016 and 2017 training sets\n",
    "train = pd.concat([df_prop_2016, df_prop_2017], axis=0, ignore_index=True)\n",
    "print(f\"Combined training set size: {len(train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 38.60 Mb (16.3% reduction)\n",
      "CPU times: user 9.57 s, sys: 104 ms, total: 9.67 s\n",
      "Wall time: 9.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reduce Size\n",
    "train = reduce_mem_usage(train)\n",
    "\n",
    "# Write training DataFrame to CSV\n",
    "train.to_csv('data/train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
