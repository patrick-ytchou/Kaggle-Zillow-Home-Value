{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# display set up\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for memory reduction\n",
    "@nb.jit()\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1022.05 Mb (75.1% reduction)\n",
      "Mem. usage decreased to 1022.05 Mb (75.1% reduction)\n",
      "Mem. usage decreased to 58.76 Mb (75.2% reduction)\n",
      "CPU times: user 4min 10s, sys: 3min 3s, total: 7min 14s\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prop_2016 = pd.read_csv('data/prop_2016.csv')\n",
    "prop_2016 = reduce_mem_usage(prop_2016)\n",
    "\n",
    "prop_2017 = pd.read_csv('data/prop_2017.csv')\n",
    "prop_2017 = reduce_mem_usage(prop_2017)\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6cafc5001416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m               'storytypeid', 'typeconstructiontypeid', 'geo_cluster']\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_2017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6cafc5001416>\u001b[0m in \u001b[0;36mencoding\u001b[0;34m(train, prop_2016, prop_2017, categorical_features)\u001b[0m\n\u001b[1;32m     13\u001b[0m                       prop_2017[categorical_features]], ignore_index=True)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_invariant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categorical_list' is not defined"
     ]
    }
   ],
   "source": [
    "def encoding(train, prop_2016, prop_2017, categorical_features):\n",
    "    \"\"\"\n",
    "    To limit the value to small numbers since values in categorical_feature is suggested to be small.\n",
    "    Now the number denoting missing values will be 0 instead of -1.\n",
    "    \"\"\"\n",
    "    from category_encoders import BinaryEncoder\n",
    "    \n",
    "    split_index = [len(train), len(prop_2016), len(prop_2017)]\n",
    "    \n",
    "    \n",
    "    prop = pd.concat([train[categorical_features],\n",
    "                      prop_2016[categorical_features], \n",
    "                      prop_2017[categorical_features]], ignore_index=True)\n",
    "    \n",
    "    encoder = BinaryEncoder(cols=categorical_list, drop_invariant = True)\n",
    "    transformed = encoder.fit_transform(prop)\n",
    "    train_be = transformed.iloc[: split_index[0]]\n",
    "    prop_2016_be = transformed.iloc[split_index[0] : split_index[0]+split_index[1]]\n",
    "    prop_2017_be = transformed.iloc[split_index[0]+split_index[1] : ]\n",
    "    \n",
    "\n",
    "    train = pd.concat([train.drop(columns=categorical_features), train_be], axis = 1)\n",
    "    prop_2016 = pd.concat([prop_2016.drop(columns=categorical_features), prop_2016_be], axis = 1)\n",
    "    prop_2017 = pd.concat([prop_2017.drop(columns=categorical_features), prop_2017_be], axis = 1)\n",
    "\n",
    "    \n",
    "    del prop, transformed, train_be, prop_2016_be, prop_2017_be\n",
    "    gc.collect()\n",
    "    return(train, prop_2016, prop_2017)\n",
    "\n",
    "# Do encoding for cateogircal features\n",
    "categorical_features = ['airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid',\n",
    "              'fips', 'heatingorsystemtypeid', 'propertycountylandusecode', 'propertylandusetypeid',\n",
    "              'propertyzoningdesc', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip',\n",
    "              'storytypeid', 'typeconstructiontypeid', 'geo_cluster']\n",
    "\n",
    "train, prop_2016, prop_2017 = encoding(train, prop_2016, prop_2017, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Numpy matrices\n",
    "X = train.drop(columns='logerror')\n",
    "y = train.logerror.values\n",
    "\n",
    "# Specify feature names\n",
    "feature_names = [col for col in X.columns]\n",
    "\n",
    "# Get categorical features\n",
    "categorical_indices = []\n",
    "for i, n in enumerate(X.columns):\n",
    "    if n in categorical_features:\n",
    "        categorical_indices.append(i)\n",
    "\n",
    "# Perform shuffled train/test split\n",
    "# np.random.seed(910)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Keep outlier values (roughly outside 99% percentile) out of the training dataset\n",
    "# outlier_threshold = 0.4\n",
    "# mask = (abs(y_train) <= outlier_threshold)\n",
    "# X_train = X_train.iloc[mask, :]\n",
    "# y_train = y_train[mask]\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_test.shape))\n",
    "print(\"y_val shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will conduct bayesian hyperparameter tuning with the help of the package `HyperOpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": Categorical([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]),\n",
    "    \"max_depth\":  Integer(1, 100),\n",
    "    \"min_samples_leaf\": Integer(1, 10),\n",
    "    \"min_samples_split\": Integer(1, 15),\n",
    "    \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "    \"bootstrap\": Categorical([True, False]),\n",
    "}\n",
    "\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# define the search\n",
    "search = BayesSearchCV(\n",
    "    estimator=clf,\n",
    "    search_spaces=params,\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    "    n_iter=300,\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# perform the search\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# report the best result\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
    "\n",
    "space = {\n",
    "    # choose within the list\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]),\n",
    "    # quantized uniform distribution to prevent float value\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 1, 100, 5),\n",
    "    \"min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 5, 1),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 1, 15, 1),\n",
    "    # choose within the list\n",
    "    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "    # Whether or not to bootstrap\n",
    "    \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom scoring function similar to the kaggle evaluation\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    diff = np.log1p(y_true) - np.log1p(y_pred)\n",
    "    return diff**2\n",
    "\n",
    "scorer = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "\n",
    "\n",
    "# define objective function\n",
    "def hyperparameter_tuning(params):\n",
    "    clf = RandomForestRegressor(**params,n_jobs=-1)\n",
    "    loss = cross_val_score(clf, X_train, y_train ,scoring=scorer).mean()\n",
    "    return {\"loss\": loss, \"status\": STATUS_OK} ## hyperparameter minimizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trials object\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=hyperparameter_tuning,\n",
    "    space = space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=100, \n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost parameters Setting\n",
    "params = {'loss_functiomn': 'MAE',\n",
    "         'eval_metric': 'MAE',\n",
    "         'nan_mode': 'Min',\n",
    "         'iterations': 1000,\n",
    "         'l2_leaf_reg': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train CatBoost Regressor with cross-validated early-stopping\n",
    "test_pool = Pool(X_test, y_test.astype(np.float64), cat_features=categorical_indices)\n",
    "\n",
    "np.random.seed(910)\n",
    "model = CatBoostRegressor(**params)\n",
    "model.fit(X_train, y_train,\n",
    "          cat_features=categorical_indices,\n",
    "          use_best_model=True, eval_set=test_pool, early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "\n",
    "# Evaluate on train and validation sets\n",
    "print(f\"Train score: {abs(model.predict(X_train) - y_train).mean() * 100}\")\n",
    "print(f\"Test score: {abs(model.predict(X_test) - y_test).mean() * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier (if any)\n",
    "outlier_threshold = 0.4\n",
    "mask = (abs(y) <= outlier_threshold)\n",
    "X = X.iloc[mask, :]\n",
    "y = y[mask]\n",
    "\n",
    "# Train multiple models\n",
    "bags = 8\n",
    "models = []\n",
    "params['iterations'] = 1000\n",
    "for i in range(bags):\n",
    "    print(\"Start training model {}\".format(i))\n",
    "    params['random_seed'] = i\n",
    "    np.random.seed(910)\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X, y, cat_features=categorical_indices, verbose=False)\n",
    "    models.append(model)\n",
    "    \n",
    "# Sanity check (make sure scores on a small portion of the dataset are reasonable)\n",
    "for i, model in enumerate(models):\n",
    "    print(\"model {}: {}\".format(i, abs(model.predict(X_test) - y_test).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_features(prop_2016, prop_2017):\n",
    "    \"\"\"\n",
    "    Helper method that prepares prop_2016 and prop_2017 for prediction.\n",
    "    \"\"\"\n",
    "    prop_2016 = drop_features(prop_2016)\n",
    "    prop_2017 = drop_features(prop_2017)\n",
    "    \n",
    "    # Create three datetime columns that does not exist in those dataset\n",
    "    prop_2016['year'] = 0\n",
    "    prop_2017['year'] = 1\n",
    "    \n",
    "    prop_2016['month'] = 8 # randomly select one month\n",
    "    prop_2017['month'] = 8 # randomly select one month\n",
    "    \n",
    "    prop_2016['quarter'] = 3 # randomly select one quarter\n",
    "    prop_2017['quarter'] = 3 # randomly select one quarter\n",
    "    \n",
    "    # Reorder to maintain categorical indices\n",
    "    prop_2016 = prop_2016[['year', 'month', 'quarter'] + list(prop_2016.columns[:-3])]\n",
    "    prop_2017 = prop_2017[['year', 'month', 'quarter'] + list(prop_2017.columns[:-3])]\n",
    "    return(prop_2016, prop_2017)\n",
    "\n",
    "def predict_and_export(models, prop_2016, prop_2017, file_name):\n",
    "    \"\"\"\n",
    "    Helper method to make predicition and export results to csv.\n",
    "    \"\"\"\n",
    "    # Construct DataFrame for prediction results\n",
    "    submission_2016 = pd.DataFrame()\n",
    "    submission_2017 = pd.DataFrame()\n",
    "    submission_2016['ParcelId'] = prop_2016.parcelid\n",
    "    submission_2017['ParcelId'] = prop_2017.parcelid\n",
    "    \n",
    "    # Prepare dataset for prediction\n",
    "    prop_2016, prop_2017 = transform_test_features(prop_2016, prop_2017)\n",
    "    \n",
    "    # Make Prediction across multiple models\n",
    "    pred_2016, pred_2017 = [], []\n",
    "    for i, model in enumerate(models):\n",
    "        print(\"Start model {} (2016)\".format(i))\n",
    "        pred_2016.append(model.predict(prop_2016))\n",
    "        print(\"Start model {} (2017)\".format(i))\n",
    "        pred_2017.append(model.predict(prop_2017))\n",
    "    \n",
    "    # Take average across all models\n",
    "    mean_pred_2016 = np.mean(pred_2016, axis=0)\n",
    "    mean_pred_2017 = np.mean(pred_2017, axis=0)\n",
    "    \n",
    "    # Formatting for submission\n",
    "    submission_2016['201610'] = [float(format(x, '.4f')) for x in mean_pred_2016]\n",
    "    submission_2016['201611'] = submission_2016['201610']\n",
    "    submission_2016['201612'] = submission_2016['201610']\n",
    "\n",
    "    submission_2017['201710'] = [float(format(x, '.4f')) for x in mean_pred_2017]\n",
    "    submission_2017['201711'] = submission_2017['201710']\n",
    "    submission_2017['201712'] = submission_2017['201710']\n",
    "    \n",
    "    submission = pd.merge(submission_2016,submission_2017, how='inner', on='ParcelId')\n",
    "    submission.to_csv(file_name, index=False)\n",
    "    print(\"Submission Successfully Created.\")\n",
    "    return(submission, pred_2016, pred_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "file_name = 'submission/final_cat.csv'\n",
    "submission, pred_2016, pred_2017 = predict_and_export(models, prop_2016, prop_2017, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
