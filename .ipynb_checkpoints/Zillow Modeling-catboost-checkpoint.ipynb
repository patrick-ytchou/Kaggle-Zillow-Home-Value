{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# display set up\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for memory reduction\n",
    "@nb.jit()\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read DataFrames from CSV file\n",
    "prop_2016 = pd.read_csv('data/prop_2016.csv')\n",
    "# prop_2016 = reduce_mem_usage(prop_2016)\n",
    "\n",
    "prop_2017 = pd.read_csv('data/prop_2017.csv')\n",
    "# prop_2017 = reduce_mem_usage(prop_2017)\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "# train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df):\n",
    "    \"\"\"\n",
    "    Drop id columns and columns not needed.\n",
    "    \"\"\"\n",
    "    # id and label (not features)\n",
    "    drop_list = ['parcelid']\n",
    "    \n",
    "    # Too many Missing Values from EDA\n",
    "    drop_list.extend(['architecturalstyletypeid','buildingclasstypeid',\n",
    "                      'decktypeid','typeconstructiontypeid'])\n",
    "    \n",
    "    # Duplicated Columns found from EDA\n",
    "    drop_list.extend(['calculatedbathnbr','finishedsquarefeet50'])\n",
    "    \n",
    "    # Highly Correlated with other related columns\n",
    "    drop_list.extend(['fullbathcnt','censustractandblock'])\n",
    "    \n",
    "    # Drop columns with low feature importance\n",
    "    drop_list.extend(['fireplaceflag','fips','finishedsquarefeet13','poolsizesum','pooltypeid10','basementsqft',\n",
    "                     'storytypeid','yardbuildingsqft26','finishedsquarefeet6','fireplacecnt','regionidcounty',\n",
    "                      'propertyzoningdesc','propertycountylandusecode'])\n",
    "\n",
    "    return(df.drop(columns=drop_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(train, prop_2016, prop_2017, categorical_features):\n",
    "    \"\"\"\n",
    "    To limit the value to small numbers since values in categorical_feature is suggested to be small.\n",
    "    Now the number denoting missing values will be 0 instead of -1.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    prop = pd.concat([train[categorical_features],\n",
    "                      prop_2016[categorical_features], \n",
    "                      prop_2017[categorical_features]], ignore_index=True)\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        encoder = LabelEncoder().fit(prop[col].astype(str))\n",
    "        train[col] = encoder.transform(train[col].astype(str))\n",
    "        prop_2016[col] = encoder.transform(prop_2016[col].astype(str))\n",
    "        prop_2017[col] = encoder.transform(prop_2017[col].astype(str))\n",
    "    del prop\n",
    "    gc.collect()\n",
    "    return(train, prop_2016, prop_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (131489, 127)\n",
      "y_train shape: (131489,)\n",
      "X_val shape: (33578, 127)\n",
      "y_val shape: (33578,)\n"
     ]
    }
   ],
   "source": [
    "# Do encoding for cateogircal features\n",
    "categorical_features = [ 'airconditioningtypeid','buildingqualitytypeid',\n",
    "                        'hashottuborspa','heatingorsystemtypeid','pooltypeid2','pooltypeid7',\n",
    "                        'propertylandusetypeid','rawcensustractandblock',\n",
    "                        'regionidcity','regionidneighborhood','regionidzip', 'yearbuilt','assessmentyear',\n",
    "                        'taxdelinquencyflag','taxdelinquencyyear','geo_cluster']\n",
    "\n",
    "train, prop_2016, prop_2017 = encoding(train, prop_2016, prop_2017, categorical_features)\n",
    "\n",
    "# Transform to Numpy matrices\n",
    "X = drop_features(train).drop(columns='logerror')\n",
    "y = train.logerror.values\n",
    "\n",
    "# Specify feature names\n",
    "feature_names = [col for col in X.columns]\n",
    "\n",
    "# Get categorical features\n",
    "categorical_indices = []\n",
    "for i, n in enumerate(X.columns):\n",
    "    if n in categorical_features:\n",
    "        categorical_indices.append(i)\n",
    "\n",
    "# Perform shuffled train/test split\n",
    "np.random.seed(910)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Keep outlier values (roughly outside 99% percentile) out of the training dataset\n",
    "outlier_threshold = 0.4\n",
    "mask = (abs(y_train) <= outlier_threshold)\n",
    "X_train = X_train.iloc[mask, :]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_test.shape))\n",
    "print(\"y_val shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost parameters Setting\n",
    "params = {'loss_functiomn': 'MAE',\n",
    "         'eval_metric': 'MAE',\n",
    "         'nan_mode': 'Min',\n",
    "         }\n",
    "params['loss_function'] = 'MAE'\n",
    "params['eval_metric'] = 'MAE'\n",
    "params['nan_mode'] = 'Min'  # Method to handle NaN (set NaN to either Min or Max)\n",
    "params['random_seed'] = 0\n",
    "\n",
    "params['iterations'] = 1000  # default 1000, use early stopping during training\n",
    "params['learning_rate'] = 0.015  # default 0.03\n",
    "\n",
    "params['border_count'] = 254  # default 254 (alias max_bin, suggested to keep at default for best quality)\n",
    "\n",
    "params['max_depth'] = 6  # default 6 (must be <= 16, 6 to 10 is recommended)\n",
    "params['random_strength'] = 1  # default 1 (used during splitting to deal with overfitting, try different values)\n",
    "params['l2_leaf_reg'] = 5  # default 3 (used for leaf value calculation, try different values)\n",
    "params['bagging_temperature'] = 1  # default 1 (higher value -> more aggressive bagging, try different values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 5.106739281624631\n",
      "Test score: 6.86613889576145\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train CatBoost Regressor with cross-validated early-stopping\n",
    "test_pool = Pool(X_test, y_test.astype(np.float64), cat_features=categorical_indices)\n",
    "\n",
    "np.random.seed(910)\n",
    "model = CatBoostRegressor(**params)\n",
    "model.fit(X_train, y_train,\n",
    "          cat_features=categorical_indices,\n",
    "          use_best_model=True, eval_set=test_pool, early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "\n",
    "# Evaluate on train and validation sets\n",
    "print(f\"Train score: {abs(model.predict(X_train) - y_train).mean() * 100}\")\n",
    "print(f\"Test score: {abs(model.predict(X_test) - y_test).mean() * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month: 4.783591734053187\n",
      "finishedsquarefeet12: 4.148568591711493\n",
      "calculatedfinishedsquarefeet_across_regionidzip_percent: 3.079769962108928\n",
      "landtaxvaluedollarcnt_across_regionidzip_percent: 3.0153672530749738\n",
      "regionidzip: 2.8145166226373215\n",
      "rawcensustractandblock: 2.599253194871222\n",
      "taxamount_across_regionidzip_percent: 2.320293838386836\n",
      "calculatedfinishedsquarefeet_across_regionidzip_diff: 2.219683093362887\n",
      "property_tax_per_sqft_across_regionidzip_percent: 2.1477290966999485\n",
      "poolcnt: 2.0464728852543366\n"
     ]
    }
   ],
   "source": [
    "# CatBoost feature importance\n",
    "feature_importance = [(feature_names[i], value) for i, value in enumerate(model.get_feature_importance())]\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "for k, v in feature_importance[:10]:\n",
    "    print(\"{}: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model 0\n",
      "Start training model 1\n",
      "Start training model 2\n",
      "Start training model 3\n",
      "Start training model 4\n",
      "Start training model 5\n",
      "Start training model 6\n",
      "Start training model 7\n",
      "model 0: 6.806486062415555\n",
      "model 1: 6.807822975141546\n",
      "model 2: 6.8120287008259295\n",
      "model 3: 6.810470335163292\n",
      "model 4: 6.8080887912448205\n",
      "model 5: 6.81278833983821\n",
      "model 6: 6.808338311486676\n",
      "model 7: 6.806888600212144\n"
     ]
    }
   ],
   "source": [
    "# Remove outlier (if any)\n",
    "outlier_threshold = 0.4\n",
    "mask = (abs(y) <= outlier_threshold)\n",
    "X = X.iloc[mask, :]\n",
    "y = y[mask]\n",
    "\n",
    "# Train multiple models\n",
    "bags = 8\n",
    "models = []\n",
    "params['iterations'] = 1000\n",
    "for i in range(bags):\n",
    "    print(\"Start training model {}\".format(i))\n",
    "    params['random_seed'] = i\n",
    "    np.random.seed(910)\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X, y, cat_features=categorical_indices, verbose=False)\n",
    "    models.append(model)\n",
    "    \n",
    "# Sanity check (make sure scores on a small portion of the dataset are reasonable)\n",
    "for i, model in enumerate(models):\n",
    "    print(\"model {}: {}\".format(i, abs(model.predict(X_test) - y_test).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_features(prop_2016, prop_2017):\n",
    "    \"\"\"\n",
    "    Helper method that prepares 2016 and 2017 properties features for inference\n",
    "    \"\"\"\n",
    "    prop_2016 = drop_features(prop_2016)\n",
    "    prop_2017 = drop_features(prop_2017)\n",
    "    \n",
    "    # Create three datetime columns that are not originated from prop dataset\n",
    "    prop_2016['year'] = 0\n",
    "    prop_2017['year'] = 1\n",
    "    \n",
    "    prop_2016['month'] = 10\n",
    "    prop_2017['month'] = 10\n",
    "    \n",
    "    prop_2016['quarter'] = 4\n",
    "    prop_2017['quarter'] = 4\n",
    "    \n",
    "    # Reorder to maintain categorical indices\n",
    "    prop_2016 = prop_2016[['year', 'month', 'quarter'] + list(prop_2016.columns[:-3])]\n",
    "    prop_2017 = prop_2017[['year', 'month', 'quarter'] + list(prop_2017.columns[:-3])]\n",
    "    return(prop_2016, prop_2017)\n",
    "\n",
    "def predict_and_export(models, prop_2016, prop_2017, file_name):\n",
    "    \"\"\"\n",
    "    Helper method to make predicition and export results to csv.\n",
    "    \"\"\"\n",
    "    # Construct DataFrame for prediction results\n",
    "    submission_2016 = pd.DataFrame()\n",
    "    submission_2017 = pd.DataFrame()\n",
    "    submission_2016['ParcelId'] = prop_2016.parcelid\n",
    "    submission_2017['ParcelId'] = prop_2017.parcelid\n",
    "    \n",
    "    prop_2016, prop_2017 = transform_test_features(prop_2016, prop_2017)\n",
    "    \n",
    "    pred_2016, pred_2017 = [], []\n",
    "    for i, model in enumerate(models):\n",
    "        print(\"Start model {} (2016)\".format(i))\n",
    "        pred_2016.append(model.predict(prop_2016))\n",
    "        print(\"Start model {} (2017)\".format(i))\n",
    "        pred_2017.append(model.predict(prop_2017))\n",
    "    \n",
    "    # Take average across all models\n",
    "    mean_pred_2016 = np.mean(pred_2016, axis=0)\n",
    "    mean_pred_2017 = np.mean(pred_2017, axis=0)\n",
    "    \n",
    "    submission_2016['201610'] = [float(format(x, '.4f')) for x in mean_pred_2016]\n",
    "    submission_2016['201611'] = submission_2016['201610']\n",
    "    submission_2016['201612'] = submission_2016['201610']\n",
    "\n",
    "    submission_2017['201710'] = [float(format(x, '.4f')) for x in mean_pred_2017]\n",
    "    submission_2017['201711'] = submission_2017['201710']\n",
    "    submission_2017['201712'] = submission_2017['201710']\n",
    "    \n",
    "    submission = submission_2016.merge(how='inner', right=submission_2017, on='ParcelId')\n",
    "    \n",
    "    print(\"Length of submission DataFrame: {}\".format(len(submission)))\n",
    "    print(\"Submission header:\")\n",
    "    print(submission.head())\n",
    "    submission.to_csv(file_name, index=False)\n",
    "    return(submission, pred_2016, pred_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model 0 (2016)\n",
      "Start model 0 (2017)\n",
      "Start model 1 (2016)\n",
      "Start model 1 (2017)\n",
      "Start model 2 (2016)\n",
      "Start model 2 (2017)\n",
      "Start model 3 (2016)\n",
      "Start model 3 (2017)\n",
      "Start model 4 (2016)\n",
      "Start model 4 (2017)\n",
      "Start model 5 (2016)\n",
      "Start model 5 (2017)\n",
      "Start model 6 (2016)\n",
      "Start model 6 (2017)\n",
      "Start model 7 (2016)\n",
      "Start model 7 (2017)\n",
      "Length of submission DataFrame: 2985217\n",
      "Submission header:\n",
      "   ParcelId     201610     201611     201612     201710     201711     201712\n",
      "0  10754147  1.690e-02  1.690e-02  1.690e-02  1.830e-02  1.830e-02  1.830e-02\n",
      "1  10759547  1.200e-02  1.200e-02  1.200e-02  1.530e-02  1.530e-02  1.530e-02\n",
      "2  10843547  5.100e-03  5.100e-03  5.100e-03  3.900e-03  3.900e-03  3.900e-03\n",
      "3  10859147  2.010e-02  2.010e-02  2.010e-02  2.040e-02  2.040e-02  2.040e-02\n",
      "4  10879947  9.000e-04  9.000e-04  9.000e-04  5.000e-04  5.000e-04  5.000e-04\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "file_name = 'submission/final_cat_2.csv'\n",
    "submission, pred_2016, pred_2017 = predict_and_export(models, prop_2016, prop_2017, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove outliers (if any) from training data\n",
    "# outlier_threshold = 0.4\n",
    "# mask = (abs(lgb_y) <= outlier_threshold)\n",
    "# lgb_X = lgb_X[mask, :]\n",
    "# lgb_y = lgb_y[mask]\n",
    "# lgb_train_set = lgb.Dataset(lgb_X, label=lgb_y, feature_name=feature_names)\n",
    "# print(\"lgb_X: {}\".format(lgb_X.shape))\n",
    "# print(\"lgb_y: {}\".format(lgb_y.shape))\n",
    "\n",
    "# del params['early_stopping_rounds']\n",
    "# del params['feature_fraction_seed']\n",
    "# del params['bagging_seed']\n",
    "# params['num_boost_round'] = 1250\n",
    "\n",
    "# # Train multiple models\n",
    "# iters = 5\n",
    "# models = []\n",
    "# for i in range(iters):\n",
    "#     print(f\"Start training model {i}\")\n",
    "#     params['seed'] = i\n",
    "#     model = lgb.train(params, lgb_train_set, verbose_eval=False, categorical_feature=categorical_indices)\n",
    "#     models.append(model)\n",
    "    \n",
    "# # Sanity check (make sure scores on a small portion of the dataset are reasonable)\n",
    "# for i, model in enumerate(models):\n",
    "#     print(\"model {}: {}\".format(i, abs(model.predict(X_val) - y_val).mean() * 100))\n",
    "\n",
    "# # Save the trained models to disk\n",
    "# save_models(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
